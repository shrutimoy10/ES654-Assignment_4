# -*- coding: utf-8 -*-
"""linear_regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dSwJGtzoxAFZlW8itoyHW0NtJSze1Pk7
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import jax

np.random.seed(45)

class LinearRegression():
  def __init__(self):
    # Initialize relevant variables
    '''
        :param fit_intercept: Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).
    '''
    self.fit_intercept = fit_intercept 
    self.coef_ = None #Replace with numpy array or pandas series of coefficients learned using using the fit methods
    self.all_coef=pd.DataFrame([]) # Stores the thetas for every iteration (theta vectors appended) (for the iterative methods)
    pass
  

  def fit_sklearn_LR(self):
    # Solve the linear regression problem by calling Linear Regression
    # from sklearn, with the relevant parameters
    pass
  
  def fit_normal_equations(self):
    # Solve the linear regression problem using the closed form solution
    # to the normal equation for minimizing ||Wx - y||_2^2
    pass

  def fit_SVD(self):
    # Solve the linear regression problem using the SVD of the 
    # coefficient matrix
    pass

  def mse_loss(self):                
    # Compute the MSE loss with the learned model
    pass

  def compute_gradient(self, penalty):
    # Compute the analytical gradient (in vectorized form) of the 
    # 1. unregularized mse_loss,  and 
    # 2. mse_loss with ridge regularization
    # penalty :  specifies the regularization used  , 'l2' or unregularized
    pass

  def compute_jax_gradient(self):
    # Compute the gradient of the 
    # 1. unregularized mse_loss, 
    # 2. mse_loss with LASSO regularization and 
    # 3. mse_loss with ridge regularization, using JAX 
    # penalty :  specifies the regularization used , 'l1' , 'l2' or unregularized
    pass

  def fit_gradient_descent(self, batch_size, gradient_type, penalty_type, num_iters=20, lr=0.01):
    # Implement batch gradient descent for linear regression (should unregularized as well as 'l1' and 'l2' regularized objective)
    # batch_size : Number of training points in each batch
    # num_iters : Number of iterations of gradient descent
    # lr : Default learning rate
    # gradient_type : manual or JAX gradients
    # penalty_type : 'l1', 'l2' or unregularized
    pass



  def fit_SGD_with_momentum(self, penalty='l2', beta=0.9):
    # Solve the linear regression problem using sklearn's implementation of SGD
    # penalty: refers to the type of regularization used (ridge)
    pass

  def predict(self, X):
    # Funtion to run the LinearRegression on a test data point
    pass


  def plot_surface(self, X, y, theta_0, theta_1):
    '''
    Function to plot RSS (residual sum of squares) in 3D. A surface plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1 by a
    red dot. Uses self.coef_ to calculate RSS. Plot must indicate error as the title.
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to indicate RSS #pd Series of all theta_0
      :param theta_1: Value of theta_1 for which to indicate RSS #pd Series of all theta_1
      :return matplotlib figure plotting RSS
    '''
    pass

  def plot_line_fit(self, X, y, theta_0, theta_1):
    """
    Function to plot fit of the line (y vs. X plot) based on chosen value of theta_0, theta_1. Plot must
    indicate theta_0 and theta_1 as the title.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting line fit
    """
    pass


  def plot_contour(self, X, y, theta_0, theta_1):
    """
    Plots the RSS as a contour plot. A contour plot is obtained by varying
    theta_0 and theta_1 over a range. Indicates the RSS based on given value of theta_0 and theta_1, and the
    direction of gradient steps. Uses self.coef_ to calculate RSS.
      :param X: pd.DataFrame with rows as samples and columns as features (shape: (n_samples, n_features))
      :param y: pd.Series with rows corresponding to output (shape: (n_samples,))
      :param theta_0: Value of theta_0 for which to plot the fit
      :param theta_1: Value of theta_1 for which to plot the fit
      :return matplotlib figure plotting the contour
    """
    pass